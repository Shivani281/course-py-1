{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e34662-4c18-46dd-b6cb-bd4e694629e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "To calculate the probability that an employee is a smoker given that he/she uses the health insurance plan, you can use conditional probability. In this case, you want to find \n",
    "(Smoker∣Uses Insurance)P(Smoker∣Uses Insurance), which is the probability of being a smoker given that the employee uses the insurance.\n",
    "\n",
    "You can use Bayes' theorem for conditional probability:\n",
    "(Smoker∣Uses Insurance)=(Uses Insurance∣Smoker)⋅(Smoker)\n",
    "(Uses Insurance)\n",
    "P(Smoker∣Uses Insurance)= \n",
    "P(Uses Insurance)\n",
    "P(Uses Insurance∣Smoker)⋅P(Smoker)\n",
    "​\n",
    " \n",
    "\n",
    "Given the information provided:\n",
    "(Uses Insurance)=0.70\n",
    "P(Uses Insurance)=0.70 (probability of using insurance)\n",
    "(Smoker)=0.40\n",
    "P(Smoker)=0.40 (probability of being a smoker)\n",
    "(Uses Insurance∣Smoker)\n",
    "P(Uses Insurance∣Smoker) is not directly given, but it can be interpreted as the probability of using insurance if you are a smoker, which you might assume is also 0.70 based on the provided information (70% of employees use the insurance plan).\n",
    "Now, plug in the values:\n",
    "(Smoker∣Uses Insurance)\n",
    "=0.70⋅0.40\n",
    "0.70=0.40\n",
    "P(Smoker∣Uses Insurance)= 0.70\n",
    "0.70⋅0.40=0.40\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.40 or 40%.\n",
    "\n",
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes classifier used for text classification and other discrete data classification tasks. Here are the key differences between them:\n",
    "\n",
    "Input Data Type:\n",
    "\n",
    "Bernoulli Naive Bayes: It is suitable for binary or boolean data, where each feature represents the presence (1) or absence (0) of a particular attribute or word in a document. It assumes that features are binary, which is often the case in spam detection (word present or not).\n",
    "Multinomial Naive Bayes: It is designed for discrete data with multiple categories or levels. In text classification, it is commonly used when features represent word counts or term frequencies (integer values).\n",
    "Feature Representation:\n",
    "\n",
    "Bernoulli Naive Bayes: It models the presence or absence of each feature in a document but doesn't consider the frequency of occurrence. It uses binary values (0 or 1).\n",
    "Multinomial Naive Bayes: It takes into account the frequency or counts of features. It is suitable when you have data like word counts or term frequencies, which can be non-binary.\n",
    "Probability Calculation:\n",
    "\n",
    "Bernoulli Naive Bayes: It calculates the probability of a document belonging to a class based on the presence or absence of features in the document. It typically uses the Bernoulli distribution to estimate probabilities.\n",
    "Multinomial Naive Bayes: It calculates probabilities based on the frequency or counts of features in a document. It often uses the multinomial distribution for probability estimation.\n",
    "Use Cases:\n",
    "\n",
    "Bernoulli Naive Bayes: It is commonly used for text classification problems where you want to determine whether certain words or features are present in a document (e.g., spam detection).\n",
    "Multinomial Naive Bayes: It is widely used in text classification tasks where you consider the frequency of words or features (e.g., sentiment analysis, topic categorization).\n",
    "In summary, the choice between Bernoulli and Multinomial Naive Bayes depends on the nature of your data and the specific problem you are trying to solve. If you are working with binary data or focusing on the presence/absence of features, Bernoulli Naive Bayes may be more appropriate. If you have discrete, count-based data like word frequencies, Multinomial Naive Bayes is a better choice.\n",
    "\n",
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Bernoulli Naive Bayes handles missing values in a straightforward manner. When using Bernoulli Naive Bayes for classification, missing values are typically treated as if they are absent or as zeros. This means that if a particular feature is missing for a data point, it is considered as if that feature is not present (or set to zero) when calculating probabilities.\n",
    "\n",
    "Here's how Bernoulli Naive Bayes handles missing values:\n",
    "\n",
    "Training Phase: During the training phase, the algorithm estimates probabilities based on the presence or absence of features. If a feature is missing for some data points in the training set, those data points are treated as if the feature is not present (i.e., set to zero) when calculating probabilities. The algorithm still learns from the available data.\n",
    "\n",
    "Testing Phase: In the testing phase, when classifying new data points, the algorithm similarly handles missing values by assuming that missing features are not present (set to zero). It calculates the likelihood of observing the features given the class and incorporates this into the classification decision.\n",
    "\n",
    "Impact on Performance: Missing values can affect the performance of Bernoulli Naive Bayes, especially if there is a significant amount of missing data for certain features. If a feature is frequently missing and is informative for classification, its absence in the presence of missing data can lead to biased or inaccurate predictions.\n",
    "\n",
    "To mitigate the impact of missing values in Bernoulli Naive Bayes, it's essential to consider data preprocessing techniques such as data imputation (replacing missing values with estimates), feature selection (choosing relevant features), and handling imbalanced data.\n",
    "\n",
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is one of the variants of the Naive Bayes algorithm and is specifically designed for data with continuous (real-valued) features. It assumes that the data in each class follows a Gaussian (normal) distribution.\n",
    "\n",
    "In multi-class classification, the goal is to classify data points into one of several possible classes or categories. Gaussian Naive Bayes can be adapted for multi-class classification by extending the algorithm to handle multiple classes.\n",
    "\n",
    "Here's how Gaussian Naive Bayes is used for multi-class classification:\n",
    "\n",
    "Modeling Class Distributions: In a multi-class problem, you have more than two classes. Gaussian Naive Bayes models the probability distribution of each class using Gaussian distributions. For each class, it estimates the mean and variance for each feature.\n",
    "\n",
    "Calculating Class Probabilities: When classifying a new data point, the algorithm calculates the probability of the data point belonging to each class using the Gaussian probability density function (PDF) based on the mean and variance estimated for each class. The class with the highest probability is assigned as the predicted class for the data point.\n",
    "\n",
    "One-vs-Rest (OvR) Strategy: One common approach to multi-class classification using Gaussian Naive Bayes is the one-vs-rest (OvR) strategy. In OvR, a separate binary classifier is trained for each class, treating it as the positive class and all other classes as the negative class. During prediction, the class with the highest probability from the binary classifiers is selected as the final prediction.\n",
    "\n",
    "Scikit-Learn: In scikit-learn, you can use the GaussianNB class for Gaussian Naive Bayes classification. It naturally supports multi-class classification, and you can specify the number of classes when creating an instance of the classifier.\n",
    "\n",
    "Here's an example of how to use Gaussian Naive Bayes for multi-class classification in scikit-learn:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset (a multi-class dataset)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "In this example, Gaussian Naive Bayes is used to classify the Iris dataset, which has three classes (setosa, versicolor, and virginica). The classifier naturally handles multi-class classification.\n",
    "\n",
    "Q5. Assignment:\n",
    "\n",
    "For the assignment, you are tasked with implementing three variants of Naive Bayes classifiers (Bernoulli, Multinomial, and Gaussian) using the scikit-learn library in Python and evaluating their performance on the \"Spambase Data Set\" from the UCI Machine Learning Repository.\n",
    "\n",
    "Here is a high-level outline of the steps to follow:\n",
    "\n",
    "Data Preparation:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the provided link.\n",
    "Load and preprocess the dataset, including any necessary data transformations and splitting it into features and labels.\n",
    "Implementation:\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using scikit-learn's respective classes.\n",
    "Train each classifier using 10-fold cross-validation.\n",
    "Performance Evaluation:\n",
    "\n",
    "For each classifier, calculate and report the following performance metrics:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1-score\n",
    "Discussion:\n",
    "\n",
    "Discuss the results you obtained. Compare the performance of the three Naive Bayes variants.\n",
    "Explain why a particular variant performed better or worse in this specific classification task.\n",
    "Identify any limitations or challenges you observed in using Naive Bayes for spam classification.\n",
    "Conclusion:\n",
    "\n",
    "Summarize your findings from the evaluation.\n",
    "Provide suggestions for future work or improvements in spam classification techniques.\n",
    "Make sure to create your assignment in a Jupyter Notebook and upload it to a public GitHub repository. You can share the GitHub repository link through your dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
