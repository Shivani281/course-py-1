{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3cf4a-f541-42ea-b829-c67e16f667db",
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform the tasks you've described, you'll need to use Python and some popular libraries for machine learning and data visualization, such as scikit-learn and Matplotlib. Here's a step-by-step guide to accomplish each of your tasks:\n",
    "\n",
    "Note: Before you begin, make sure to download the dataset and place it in your working directory or provide the appropriate path to the dataset in your code.\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Q1. Preprocess the dataset\n",
    "# Load the dataset\n",
    "data = pd.read_csv('heart_disease_dataset.csv')\n",
    "\n",
    "# Handle missing values (if any)\n",
    "# Assuming missing values are denoted by NaN\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Encode categorical variables (if any)\n",
    "# If there are categorical variables, you can use techniques like one-hot encoding.\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Scale numerical features (if necessary)\n",
    "# Scaling is not always necessary for tree-based models like Random Forests.\n",
    "\n",
    "# Q2. Split the dataset into training and test sets (70% - 30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Q3. Train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Q4. Evaluate the performance of the model on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Q5. Get feature importance scores and visualize them\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "top_features = np.argsort(feature_importances)[::-1][:5]\n",
    "top_feature_names = X.columns[top_features]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(5), feature_importances[top_features], tick_label=top_feature_names)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.title('Top 5 Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "# Q6. Tune hyperparameters using Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Q7. Report the best hyperparameters and evaluate the tuned model\n",
    "y_pred_tuned = best_rf_classifier.predict(X_test)\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "precision_tuned = precision_score(y_test, y_pred_tuned)\n",
    "recall_tuned = recall_score(y_test, y_pred_tuned)\n",
    "f1_tuned = f1_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(f\"Accuracy (Tuned Model): {accuracy_tuned}\")\n",
    "print(f\"Precision (Tuned Model): {precision_tuned}\")\n",
    "print(f\"Recall (Tuned Model): {recall_tuned}\")\n",
    "print(f\"F1 Score (Tuned Model): {f1_tuned}\")\n",
    "\n",
    "# Compare performance with default model\n",
    "print(f\"Accuracy (Default Model): {accuracy}\")\n",
    "print(f\"Precision (Default Model): {precision}\")\n",
    "print(f\"Recall (Default Model): {recall}\")\n",
    "print(f\"F1 Score (Default Model): {f1}\")\n",
    "\n",
    "# Q8. Visualize decision boundaries (assuming 2 important features)\n",
    "# Select two most important features\n",
    "feature1 = top_feature_names[0]\n",
    "feature2 = top_feature_names[1]\n",
    "\n",
    "# Extract those features from the dataset\n",
    "X_subset = X[[feature1, feature2]]\n",
    "\n",
    "# Train a random forest classifier on the subset\n",
    "rf_classifier_subset = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_classifier_subset.fit(X_subset, y)\n",
    "\n",
    "# Create a scatter plot of the two features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_subset[feature1], X_subset[feature2], c=y, cmap='coolwarm', marker='o')\n",
    "plt.xlabel(feature1)\n",
    "plt.ylabel(feature2)\n",
    "\n",
    "# Create a mesh grid for decision boundaries\n",
    "xx, yy = np.meshgrid(np.arange(X_subset[feature1].min() - 1, X_subset[feature1].max() + 1, 0.01),\n",
    "                     np.arange(X_subset[feature2].min() - 1, X_subset[feature2].max() + 1, 0.01))\n",
    "\n",
    "# Predict for each point in the mesh grid\n",
    "Z = rf_classifier_subset.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.3)\n",
    "plt.title('Decision Boundaries of Random Forest Classifier')\n",
    "plt.show()\n",
    "\n",
    "# Q8. Interpretation\n",
    "# Analyze the decision boundaries and discuss insights and limitations of the model.\n",
    "This code covers the entire process from preprocessing the dataset to evaluating the model and visualizing the decision boundaries. You can adapt it to your specific dataset and requirements. Make sure you have the required libraries installed using pip install scikit-learn matplotlib pandas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
