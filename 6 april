{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa25cd-a290-4145-a1e6-779c6366842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) can be defined as follows:\n",
    "\n",
    "Given a training dataset with feature vectors X and corresponding binary labels y (1 for positive class and -1 for negative class), the goal of a linear SVM is to find a hyperplane represented by the equation:\n",
    "\n",
    "w·x + b = 0\n",
    "\n",
    "where:\n",
    "\n",
    "w is the weight vector perpendicular to the hyperplane.\n",
    "x is the input feature vector.\n",
    "b is the bias term.\n",
    "The decision function for classifying a new data point x_new is determined by:\n",
    "\n",
    "f(x_new) = sign(w·x_new + b)\n",
    "\n",
    "The objective is to find the weight vector w and bias term b that maximize the margin (distance) between the two classes while minimizing classification errors.\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "The objective function of a linear SVM, also known as the margin-maximizing objective or the hinge loss, aims to find the hyperplane that maximizes the margin between the two classes while minimizing classification errors. It can be formulated as follows:\n",
    "\n",
    "Minimize: 1/2 * ||w||^2 + C * Σ(max(0, 1 - y_i(w·x_i + b)))\n",
    "\n",
    "Subject to: y_i(w·x_i + b) ≥ 1 for all training samples (i)\n",
    "\n",
    "In this objective function:\n",
    "\n",
    "w is the weight vector perpendicular to the hyperplane.\n",
    "b is the bias term.\n",
    "x_i represents each training feature vector.\n",
    "y_i is the corresponding binary label (1 or -1).\n",
    "C is the regularization parameter that balances margin maximization and error minimization.\n",
    "The first term 1/2 * ||w||^2 represents the margin maximization component, where ||w|| is the L2 norm of the weight vector. It encourages a larger margin between the two classes.\n",
    "\n",
    "The second term Σ(max(0, 1 - y_i(w·x_i + b))) is the error minimization component. It accounts for misclassified samples and penalizes them based on the margin they violate. The \"max(0, ...)\" function ensures that errors are only penalized when they exceed the margin.\n",
    "\n",
    "The objective is to find the optimal values of w and b that minimize this combined objective function.\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "The kernel trick in Support Vector Machines (SVMs) is a mathematical technique that allows SVMs to handle non-linearly separable data by implicitly mapping the input features into a higher-dimensional space. This higher-dimensional space often makes it possible to find a hyperplane that linearly separates the data, even when the original feature space cannot.\n",
    "\n",
    "The kernel trick works by introducing a kernel function K(x, x') that computes the dot product of the feature vectors in the higher-dimensional space without explicitly computing the transformation. The key equation is:\n",
    "\n",
    "K(x, x') = φ(x)·φ(x')\n",
    "\n",
    "Where:\n",
    "\n",
    "K(x, x') is the kernel function.\n",
    "φ(x) represents the mapping of the input feature vector x into the higher-dimensional space.\n",
    "Commonly used kernel functions include:\n",
    "\n",
    "Linear Kernel (K(x, x') = x·x'): Equivalent to a linear SVM in the original feature space.\n",
    "\n",
    "Polynomial Kernel (K(x, x') = (γ(x·x') + r)^d): Maps data to a polynomial feature space of degree d.\n",
    "\n",
    "Radial Basis Function (RBF) Kernel (K(x, x') = exp(-γ||x - x'||^2)): Maps data to an infinite-dimensional space using Gaussian basis functions.\n",
    "\n",
    "Sigmoid Kernel (K(x, x') = tanh(γ(x·x') + r)): Maps data using a hyperbolic tangent function.\n",
    "\n",
    "The choice of kernel function depends on the nature of the data and the problem at hand. The kernel trick allows SVMs to capture complex, non-linear decision boundaries effectively.\n",
    "\n",
    "Q4. What is the role of support vectors in SVM? Explain with an example.\n",
    "\n",
    "Support vectors are the data points from the training dataset that play a critical role in defining the decision boundary (hyperplane) of a Support Vector Machine (SVM). They are the closest data points to the decision boundary and have a direct influence on the placement and orientation of the hyperplane. Here's their role explained with an example:\n",
    "\n",
    "Example: Binary Classification of Iris Flowers\n",
    "\n",
    "Suppose you have a binary classification problem to distinguish between two species of iris flowers based on sepal length and sepal width. You have two classes: Setosa (positive class) and Versicolor (negative class).\n",
    "\n",
    "Training Data: You have a training dataset with sepal length and sepal width measurements for various iris flowers and their corresponding labels (Setosa or Versicolor).\n",
    "\n",
    "SVM Training: You train a linear SVM on this dataset to find the optimal hyperplane that separates the two classes.\n",
    "\n",
    "Support Vectors: During the training process, the SVM identifies a subset of data points as support vectors. These are the data points that are closest to the hyperplane or lie within the margin.\n",
    "\n",
    "For example, some Setosa flowers with sepal lengths and widths close to the decision boundary are selected as support vectors. Similarly, some Versicolor flowers near the boundary are also chosen.\n",
    "Defining the Hyperplane: The position and orientation of the hyperplane are determined by the support vectors. The hyperplane is positioned to maximize the margin between the two classes while ensuring that it is as far away as possible from the support vectors.\n",
    "\n",
    "Classification: When you want to classify a new iris flower, the SVM uses the support vectors and the hyperplane to make predictions. It calculates the distance of the new data point from the hyperplane. The sign of this distance, along with the position of the hyperplane, determines the predicted class label.\n",
    "\n",
    "If the new data point is on the same side of the hyperplane as most of the support vectors for Setosa, it is classified as Setosa.\n",
    "If it is on the same side as most of the support vectors for Versicolor, it is classified as Versicolor.\n",
    "Support vectors are crucial because they represent the most challenging and informative\n",
    "Q5. Illustration of Hyperplane, Marginal Plane, Soft Margin, and Hard Margin in SVM:\n",
    "\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. In the context of SVM, hyperplane, marginal plane, soft margin, and hard margin are important concepts.\n",
    "\n",
    "Hyperplane: In SVM, a hyperplane is a decision boundary that separates data points of one class from data points of another class. For a binary classification problem, it is a flat affine subspace of dimension one less than the input data. The goal of SVM is to find the hyperplane that maximizes the margin between the two classes. The equation of the hyperplane is given by:\n",
    "\n",
    "w⋅x+b=0\n",
    "                                                         \n",
    "\n",
    "Where:\n",
    "w is the weight vector.\n",
    "x is the feature vector.\n",
    "b is the bias or intercept term.\n",
    "The hyperplane is shown as a straight line in 2D and a hyperplane in higher dimensions.\n",
    "\n",
    "Marginal Plane: The marginal plane in SVM represents the boundary that is equidistant from the support vectors of both classes. It is also known as the decision boundary or the optimal hyperplane. The margin is the distance between the marginal plane and the nearest support vectors.\n",
    "\n",
    "Soft Margin: SVM allows for some misclassification or data points to fall within the margin. This is done to handle noisy data or cases where the data is not perfectly separable. Soft margin SVM introduces a parameter called \n",
    "C, which controls the trade-off between maximizing the margin and allowing some misclassification. A smaller \n",
    "C value allows more misclassification, while a larger \n",
    "C value enforces a stricter margin.\n",
    "\n",
    "Hard Margin: In contrast, a hard margin SVM does not allow any misclassification. It requires that all data points be correctly classified and lie outside the margin. This can lead to overfitting when the data is not perfectly linearly separable.\n",
    "\n",
    "Here are visual representations of these concepts:\n",
    "\n",
    "Hyperplane (2D Example):\n",
    "Hyperplane\n",
    "\n",
    "Marginal Plane (2D Example):\n",
    "Marginal Plane\n",
    "\n",
    "Soft Margin vs. Hard Margin (2D Example):\n",
    "Soft Margin vs. Hard Margin\n",
    "\n",
    "Q6. SVM Implementation through Iris dataset:\n",
    "\n",
    "Sure, here's how you can implement a linear SVM classifier using Python and the Iris dataset from scikit-learn. We'll also explore the impact of different values of the regularization parameter \n",
    "C on the model's performance.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Take the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]  # Different values of C to try\n",
    "accuracies = []\n",
    "\n",
    "for C in C_values:\n",
    "    clf = svm.SVC(kernel='linear', C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"C={C}, Accuracy={accuracy}\")\n",
    "\n",
    "# Plot decision boundaries using two features\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.title('SVM Decision Boundaries')\n",
    "plt.show()\n",
    "\n",
    "# Plot the impact of different C values on accuracy\n",
    "plt.figure()\n",
    "plt.plot(C_values, accuracies, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (Regularization Parameter)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of C on SVM Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "In this code, we load the Iris dataset, split it into a training and testing set, train a linear SVM classifier with different values of the regularization parameter \n",
    "C, and visualize the decision boundaries. We also plot the impact of different \n",
    "C values on accuracy. You can run this code in a Jupyter Notebook and adjust \n",
    "C values as needed to observe their effects on the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Regenerate\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
