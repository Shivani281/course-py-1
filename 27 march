{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2936be-65cd-4800-8f2e-3e329786a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. R-squared, also known as the coefficient of determination, is a statistical measure used in linear regression models to assess how well the independent variables (predictors) explain the variability in the dependent variable (target). It is a value between 0 and 1, where 0 indicates that the model does not explain any of the variability, and 1 indicates that the model explains all the variability.\n",
    "\n",
    "R-squared is calculated as follows:\n",
    "\n",
    "First, compute the total sum of squares (SST), which measures the total variability in the dependent variable.\n",
    "Then, calculate the sum of squared residuals (SSE), which measures the unexplained variability by the model.\n",
    "Finally, R-squared is obtained by subtracting SSE from SST and dividing by SST: R-squared = 1 - (SSE / SST).\n",
    "R-squared represents the proportion of the variance in the dependent variable that is explained by the independent variables. Higher R-squared values indicate a better fit of the model to the data.\n",
    "\n",
    "Q2. Adjusted R-squared is a modified version of R-squared that accounts for the number of predictors in the model. Unlike R-squared, adjusted R-squared penalizes the inclusion of unnecessary predictors, which can help prevent overfitting. The formula for adjusted R-squared is as follows:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "Where:\n",
    "\n",
    "n is the number of data points.\n",
    "k is the number of predictors in the model.\n",
    "Adjusted R-squared is always lower than or equal to R-squared. It increases when adding relevant predictors but decreases when adding irrelevant ones. It provides a more realistic estimate of the model's goodness of fit, especially when dealing with models with different numbers of predictors.\n",
    "\n",
    "Q3. Adjusted R-squared is more appropriate when comparing models with different numbers of predictors. It helps you select a more parsimonious model by penalizing the inclusion of excessive predictors. It's particularly useful when you want to strike a balance between model complexity and goodness of fit, as it adjusts for the trade-off between adding predictors and model fit.\n",
    "\n",
    "Q4. In the context of regression analysis:\n",
    "\n",
    "RMSE (Root Mean Squared Error) measures the square root of the average of the squared differences between the predicted and actual values. It provides a measure of the model's prediction accuracy.\n",
    "MSE (Mean Squared Error) is the average of the squared differences between predicted and actual values. It quantifies the average squared error.\n",
    "MAE (Mean Absolute Error) is the average of the absolute differences between predicted and actual values. It measures the average magnitude of errors.\n",
    "Q5. Advantages and disadvantages of these metrics:\n",
    "\n",
    "RMSE and MSE give more weight to larger errors, making them sensitive to outliers. This can be advantageous when large errors are costly but problematic when outliers are common.\n",
    "MAE is more robust to outliers because it uses absolute differences. However, it may not penalize large errors as strongly as RMSE/MSE.\n",
    "RMSE is more commonly used when you want to prioritize the reduction of large errors, while MAE is preferred when you want to minimize the impact of outliers.\n",
    "Q6. Lasso regularization is a technique used in linear regression to prevent overfitting and feature selection. It adds a penalty term to the linear regression cost function, forcing some regression coefficients to become exactly zero. This leads to sparse models where some predictors are effectively removed.\n",
    "\n",
    "Lasso differs from Ridge regularization in that Ridge only adds a penalty term based on the square of the coefficients (L2 regularization), which discourages large coefficients but doesn't force them to be exactly zero.\n",
    "\n",
    "Lasso is more appropriate when you suspect that only a subset of predictors is relevant and want automatic feature selection. Ridge is often preferred when you want to reduce the impact of multicollinearity (high correlations between predictors) without removing them entirely.\n",
    "\n",
    "Q7. Regularized linear models help prevent overfitting by adding penalty terms to the cost function, discouraging overly complex models. For example, in Ridge regularization, the penalty term is based on the square of the coefficients, limiting their magnitude. In Lasso regularization, the penalty term can force some coefficients to be exactly zero, effectively selecting a subset of features.\n",
    "\n",
    "Example: In a Ridge regression model with many highly correlated predictors, the regularization term will encourage the model to distribute coefficients more evenly among the predictors, reducing the risk of over-relying on a single predictor.\n",
    "\n",
    "Q8. Limitations of regularized linear models:\n",
    "\n",
    "They may not perform well when the relationship between predictors and the target variable is highly nonlinear.\n",
    "The choice of the regularization parameter (e.g., alpha in Ridge or Lasso) can be challenging and may require tuning.\n",
    "Interpretability can be reduced in models with many zero coefficients (Lasso) because some predictors are excluded.\n",
    "Regularized models might not work well when there are many irrelevant predictors, as they won't completely eliminate them.\n",
    "Q9. The choice of the better-performing model depends on the specific goals and requirements. RMSE and MAE measure different aspects of model performance. RMSE gives more weight to larger errors, while MAE treats all errors equally. If you prioritize minimizing large errors and the RMSE of Model A (10) is acceptable, you might choose Model A. However, if you want to minimize the impact of outliers and Model B's MAE of 8 is acceptable, you might choose Model B. There are no absolute rules; it depends on the context and the importance of different types of errors.\n",
    "\n",
    "Q10. The choice between Ridge and Lasso regularization depends on the specific problem and the characteristics of the data:\n",
    "\n",
    "Model A (Ridge regularization with alpha=0.1) tends to work well when there is multicollinearity among predictors, as it reduces the impact of correlated predictors without excluding them entirely.\n",
    "Model B (Lasso regularization with alpha=0.5) is suitable when feature selection is crucial, and you suspect that many predictors are irrelevant or redundant, as it can force some coefficients to be exactly zero.\n",
    "There are trade-offs involved: Ridge is less aggressive in feature selection, while Lasso can lead to sparser models. The choice depends on the balance between model simplicity and predictive power required for the specific problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
